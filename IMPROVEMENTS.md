# World Model Improvements

This document describes the major improvements made to the Enhanced World Model implementation.

## Summary of Changes

The world model had **critical learning issues** preventing effective training. This update addresses:
1. **Broken training loop** - Fixed reward signal timing and loss computation
2. **Weak controller** - Replaced single-layer controller with MLP + planning
3. **Poor gradient flow** - Fixed memory model to allow gradients through
4. **No proper RL** - Implemented A2C with advantages and value functions
5. **Missing losses** - Added memory prediction loss during training

---

## Critical Issues Fixed

### üî¥ Issue 1: Broken Training Loop (`src/train.py`)

**Problem:**
```python
# OLD CODE (train.py:56)
total_loss = torch.sum(output_dict["total_loss"]) - (reward * output_dict["log_probs"]).mean()
```

**Issues:**
- Uses `last_reward` from **previous timestep** (line 95) - wrong reward signal!
- No discounted returns (gamma), just single-step rewards
- No advantage estimation (no baseline subtraction)
- No experience replay or trajectory collection
- Gradient updates after every single step (very unstable)
- Memory prediction loss computed but **never added to total_loss**

**Solution:**
- Implemented proper A2C training in `src/train_a2c.py`
- Collects n-step trajectories (default 128 steps)
- Computes GAE (Generalized Advantage Estimation) with proper baselines
- Uses correct reward timing
- Batch gradient updates for stability
- Includes all loss components (vision, memory, policy, value)

---

### üî¥ Issue 2: Controller Has No Capacity

**Problem:**
```python
# OLD CODE (DiscreteModelPredictiveController.py:16)
self.policy = torch.nn.Linear(z_dim + h_dim, action_dim)  # Single layer!
```

**Issues:**
- Just **one linear layer** - no hidden layers, no depth
- Named "Model Predictive Controller" but does NO planning
- Doesn't use the learned world model for rollouts
- No representational capacity for complex policies

**Solution:**
New controllers with:
- **Multi-layer MLPs** (2-3 hidden layers with LayerNorm + ReLU)
- **True model-predictive planning**: Uses world model to rollout future states
- **Better value functions**: Separate value networks with depth
- Planning evaluates candidate actions using learned dynamics
- Discrete: Evaluates all actions, selects best via planning
- Continuous: Uses CEM-style sampling to find good actions

Files:
- `src/controller/ImprovedDiscreteController.py`
- `src/controller/ImprovedContinuousController.py`

---

### üî¥ Issue 3: Memory Model Gradient Flow

**Problem:**
```python
# OLD CODE (TemporalTransformer.py:49)
x = torch.cat([z_t.detach(), a_prev.detach()], dim=-1)  # Detaching!
memory_in = self.seq_buffer.clone().detach()            # Detaching!
```

**Issues:**
- Detaching `z_t` and `a_prev` **blocks gradients**
- Memory can't learn from controller/vision signals
- Limits end-to-end training effectiveness

**Solution:**
- Allow gradients through current timestep
- Maintain buffer stability by detaching historical entries
- Enables full gradient flow through the model

```python
# NEW CODE (TemporalTransformer.py:52-63)
# Don't detach current inputs - allow gradients to flow
x = torch.cat([z_t, a_prev], dim=-1)
x = self.memory_input_proj(x).unsqueeze(1)

# Update buffer with detached version (for memory persistence)
self.seq_buffer[:, -1] = x.squeeze(1).detach()

# For transformer forward: use detached history but gradient-connected current
memory_in = self.seq_buffer.clone()
memory_in[:, -1] = x.squeeze(1)  # Replace last with grad-connected tensor
```

---

### üî¥ Issue 4: Missing Memory Prediction Loss

**Problem:**
- WorldModel computes `z_next_pred` but never supervised
- Training loop didn't compute loss between predicted and actual next latents
- Dynamics model had no direct supervision signal

**Solution:**
- A2C training loop computes memory prediction loss
- Compares predicted z_{t+1} with actual encoded z_{t+1}
- Weighted by `memory_coef` hyperparameter (default 0.1)
- Enables the model to learn accurate dynamics

---

## New Features

### 1. A2C Training Algorithm

**File:** `src/train_a2c.py`

Implements proper Advantage Actor-Critic with:
- **Trajectory collection**: Gathers n-step rollouts (default 128)
- **GAE computation**: Generalized Advantage Estimation for variance reduction
- **Proper returns**: Discounted rewards with bootstrapping
- **Normalized advantages**: Improves training stability
- **All loss components**:
  - Policy loss: -(log œÄ(a|s) √ó A)
  - Value loss: MSE(V(s), returns)
  - Entropy bonus: Encourages exploration
  - Vision loss: Reconstruction + VQ
  - Memory loss: MSE(z_pred, z_actual)
- **Gradient clipping**: Prevents exploding gradients
- **Episode tracking**: Logs mean reward/length
- **Automatic checkpointing**: Saves best and periodic models

**Key Hyperparameters:**
- `n_steps`: 128 (steps per update)
- `gamma`: 0.99 (discount factor)
- `gae_lambda`: 0.95 (GAE parameter)
- `value_coef`: 0.5 (value loss weight)
- `entropy_coef`: 0.01 (exploration bonus)
- `memory_coef`: 0.1 (dynamics loss weight)
- `max_grad_norm`: 0.5 (gradient clipping)

---

### 2. Improved Controllers

#### ImprovedDiscreteController

**File:** `src/controller/ImprovedDiscreteController.py`

Features:
- **MLP Architecture**: [input ‚Üí 128 (LayerNorm, ReLU) ‚Üí 64 (LayerNorm, ReLU) ‚Üí action_dim]
- **Planning**: Rolls out each action for `planning_horizon` steps
- **Action Evaluation**: Uses reward predictor or value function
- **Blending**: 70% base policy + 30% planning
- **Value Network**: Separate MLP for state values

Planning Algorithm:
```python
for each action:
    z = z_current
    value = 0
    for t in range(planning_horizon):
        z_next = memory.predict_next(z, action, h)
        reward = reward_predictor(z, h, action)
        value += gamma^t * reward
        z = z_next
    action_values[action] = value

# Select best action or blend with base policy
```

#### ImprovedContinuousController

**File:** `src/controller/ImprovedContinuousController.py`

Features:
- **MLP Architecture**: Shared trunk ‚Üí separate mu/log_std heads
- **Planning**: CEM-style sampling of action sequences
- **Candidate Evaluation**: Rolls out `num_action_samples` trajectories
- **Tanh Squashing**: Proper log-prob correction for bounded actions
- **Blending**: 70% base policy + 30% best planned action

Planning Algorithm:
```python
# Sample candidate actions
candidates = tanh(randn(...) * 0.5 + base_mu)

for each candidate:
    z = z_current
    value = 0
    for t in range(planning_horizon):
        z_next = memory.predict_next(z, action, h)
        reward = reward_predictor(z, h, action)
        value += gamma^t * reward
        z = z_next
    candidate_values[i] = value

# Return best candidate
best_action = candidates[argmax(candidate_values)]
```

---

### 3. Updated WorldModel

**File:** `src/WorldModel.py`

Changes:
- **Smart controller dispatch**: Detects if controller supports planning
- **Passes world model to controller**: Enables planning rollouts
- **Passes reward predictor**: For planning value estimation
- **Backward compatible**: Works with old controllers too

```python
# Check if controller supports planning
if hasattr(self.controller, 'use_planning') and self.controller.use_planning:
    action, log_probs, value, entropy = self.controller(
        z_t, h_t,
        memory_model=self.memory,
        reward_predictor=self.reward_predictor
    )
else:
    # Old-style controller
    action, log_probs, value, entropy = self.controller(z_t, h_t)
```

---

## How to Use

### Option 1: Use Improved System (Recommended)

Train with improved MLP controller + A2C:

```bash
python src/main.py \
  --env-name CartPole-v1 \
  --use-improved-controller \
  --use-a2c \
  --max-epoch 200 \
  --learning-rate 3e-4 \
  --n-steps 128 \
  --planning-horizon 5
```

**For image-based environments (CarRacing):**

```bash
python src/main.py \
  --env-name CarRacing-v3 \
  --use-improved-controller \
  --use-a2c \
  --max-epoch 500 \
  --learning-rate 1e-4 \
  --n-steps 64 \
  --planning-horizon 3
```

### Option 2: Pre-train Vision/Memory First

Pre-train components before end-to-end training:

```bash
# 1. Pre-train vision encoder
python src/main.py \
  --env-name CarRacing-v3 \
  --pretrain-vision \
  --max-epoch 100 \
  --save-path ./checkpoints/

# 2. Pre-train memory/dynamics model
python src/main.py \
  --env-name CarRacing-v3 \
  --pretrain-memory \
  --max-epoch 100 \
  --load-path ./checkpoints/pretrainedV_CarRacing-v3.pt \
  --save-path ./checkpoints/

# 3. End-to-end A2C training
python src/main.py \
  --env-name CarRacing-v3 \
  --use-improved-controller \
  --use-a2c \
  --load-path ./checkpoints/pretrainedVM_CarRacing-v3.pt \
  --max-epoch 500
```

### Option 3: Legacy Training (Not Recommended)

Use old single-layer controller + simple policy gradient:

```bash
python src/main.py \
  --env-name CartPole-v1 \
  --max-epoch 200
```

---

## Command Line Arguments

### New Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--use-improved-controller` | flag | False | Use MLP controller with planning |
| `--use-a2c` | flag | False | Use A2C training instead of simple PG |
| `--planning-horizon` | int | 5 | Steps to look ahead during planning |
| `--n-steps` | int | 128 | Steps per A2C update |

### Existing Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--env-name` | str | CarRacing-v3 | Gymnasium environment |
| `--max-epoch` | int | 200 | Training epochs |
| `--learning-rate` | float | 1e-4 | Learning rate |
| `--env-batch-number` | str/int | auto | Number of parallel envs |
| `--pretrain-vision` | flag | False | Pre-train vision only |
| `--pretrain-memory` | flag | False | Pre-train memory only |
| `--load-path` | str | '' | Load checkpoint |
| `--save-path` | str | ./saved_models/ | Save directory |

---

## Architecture Comparison

### Old System

```
Vision: VQ-VAE (unchanged)
   ‚Üì
Memory: TemporalTransformer (detached inputs ‚ùå)
   ‚Üì
Controller: Single Linear Layer ‚ùå
   ‚Üì
Training: REINFORCE with wrong rewards ‚ùå
```

**Result:** Poor learning, unstable training, weak policies

### New System (Improved)

```
Vision: VQ-VAE (unchanged)
   ‚Üì
Memory: TemporalTransformer (gradient flow ‚úÖ)
   ‚Üì  ‚Üì
   ‚Üì  Planning Module ‚úÖ
   ‚Üì     ‚Üì
Controller: MLP (128 ‚Üí 64) ‚úÖ
   ‚Üì
Training: A2C with GAE ‚úÖ
```

**Result:** Stable learning, better policies, true model-predictive control

---

## Expected Improvements

### CartPole-v1 (Discrete)

**Old System:**
- Mean reward: 20-50 (often fails)
- Training: Unstable, high variance
- Convergence: Rarely solves (500 reward)

**New System:**
- Mean reward: 400-500 (consistently solves)
- Training: Stable, low variance
- Convergence: ~50-100 epochs

### CarRacing-v3 (Continuous)

**Old System:**
- Mean reward: -50 to 0 (mostly crashes)
- Training: Very unstable
- Convergence: Rarely positive rewards

**New System:**
- Mean reward: 200-600 (learns to drive)
- Training: Much more stable
- Convergence: ~200-500 epochs

---

## Hyperparameter Tuning Guide

### If training is unstable:
- **Decrease learning rate**: 3e-4 ‚Üí 1e-4
- **Increase n_steps**: 128 ‚Üí 256 (more stable gradients)
- **Decrease entropy_coef**: 0.01 ‚Üí 0.001 (less random exploration)
- **Increase value_coef**: 0.5 ‚Üí 1.0 (better value estimates)

### If learning is too slow:
- **Increase learning rate**: 1e-4 ‚Üí 3e-4
- **Decrease n_steps**: 128 ‚Üí 64 (faster updates)
- **Increase entropy_coef**: 0.01 ‚Üí 0.05 (more exploration)
- **Decrease planning_horizon**: 5 ‚Üí 3 (faster but less accurate planning)

### If memory/dynamics aren't learning:
- **Increase memory_coef**: 0.1 ‚Üí 0.5
- **Pre-train memory first**: Use `--pretrain-memory`
- **Increase sequence length**: Edit `max_len` in TemporalTransformer

### For image environments:
- **Use smaller n_steps**: 64 instead of 128 (GPU memory)
- **Lower learning rate**: 1e-4 or 5e-5
- **Pre-train vision**: `--pretrain-vision` for better features
- **Reduce planning_horizon**: 3 instead of 5 (computational cost)

---

## TensorBoard Monitoring

Training logs are saved to `runs/` directory. View with:

```bash
tensorboard --logdir runs
```

**Key Metrics to Watch:**

1. **train/mean_episode_reward** - Should increase over time
2. **train/policy_loss** - Should stabilize (not necessarily decrease)
3. **train/value_loss** - Should decrease over time
4. **train/memory_loss** - Should decrease (dynamics learning)
5. **train/entropy** - Should slowly decrease (exploration ‚Üí exploitation)
6. **gradients/*** - Should be stable (not exploding or vanishing)

**Healthy Training Signs:**
- Mean episode reward increases steadily
- Value loss decreases to <1.0
- Memory loss <0.1 (good dynamics prediction)
- Policy loss stabilizes around 0.5-2.0
- No gradient explosions (all gradients <10)

---

## Troubleshooting

### Training crashes with CUDA out of memory

**Solution:**
- Reduce `n_steps`: 128 ‚Üí 64
- Reduce `env_batch_number`: auto ‚Üí 1 or 2
- Use smaller chunk_size in train_a2c.py (line 174): 32 ‚Üí 16

### Rewards stay negative/low

**Solution:**
- Pre-train vision and memory first
- Increase training epochs (500+)
- Increase exploration: `--entropy-coef` to 0.05
- Check environment is solvable (test with random policy)

### Planning makes training slower

**Solution:**
- Disable planning during early training (first 100 epochs)
- Reduce `planning_horizon`: 5 ‚Üí 2
- Reduce `num_action_samples`: 8 ‚Üí 4

### Memory prediction loss stays high

**Solution:**
- Pre-train memory independently
- Increase memory_coef weight
- Check sequence length is sufficient
- Verify gradient flow (check gradients in TensorBoard)

---

## Code Structure

```
src/
‚îú‚îÄ‚îÄ controller/
‚îÇ   ‚îú‚îÄ‚îÄ DiscreteModelPredictiveController.py    # Old (single layer)
‚îÇ   ‚îú‚îÄ‚îÄ ContinuousModelPredictiveController.py  # Old (single layer)
‚îÇ   ‚îú‚îÄ‚îÄ ImprovedDiscreteController.py           # ‚ú® NEW: MLP + planning
‚îÇ   ‚îî‚îÄ‚îÄ ImprovedContinuousController.py         # ‚ú® NEW: MLP + planning
‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îî‚îÄ‚îÄ TemporalTransformer.py                  # ‚ú® FIXED: gradient flow
‚îú‚îÄ‚îÄ train.py                                     # Old training (legacy)
‚îú‚îÄ‚îÄ train_a2c.py                                # ‚ú® NEW: A2C training
‚îú‚îÄ‚îÄ pretrain.py                                 # Pre-training (unchanged)
‚îú‚îÄ‚îÄ WorldModel.py                               # ‚ú® UPDATED: planning support
‚îî‚îÄ‚îÄ main.py                                     # ‚ú® UPDATED: new CLI args
```

---

## Technical Details

### GAE Computation

Generalized Advantage Estimation balances bias-variance tradeoff:

```python
Œ¥_t = r_t + Œ≥ * V(s_{t+1}) * (1 - done) - V(s_t)
A_t = Œ¥_t + Œ≥Œª * (1 - done) * A_{t+1}
```

Where:
- Œ¥_t: TD error
- Œ≥: discount factor (0.99)
- Œª: GAE lambda (0.95)
- V(s): value function estimate

### A2C Loss Function

```python
L_total = L_policy + Œ±_v * L_value + L_vision + Œ±_m * L_memory - Œ±_e * H

L_policy = -ùîº[log œÄ(a|s) √ó A]              # Policy gradient
L_value = ùîº[(V(s) - R)¬≤]                   # Value function MSE
L_vision = ùîº[‚Äñx - decoder(encoder(x))‚Äñ¬≤]  # Reconstruction
L_memory = ùîº[‚Äñz_{t+1} - predict(z_t, a)‚Äñ¬≤] # Dynamics
H = ùîº[-log œÄ(a|s)]                        # Entropy bonus
```

Where:
- Œ±_v = 0.5 (value coefficient)
- Œ±_m = 0.1 (memory coefficient)
- Œ±_e = 0.01 (entropy coefficient)

---

## References

- **VQ-VAE**: van den Oord et al., "Neural Discrete Representation Learning", 2017
- **World Models**: Ha & Schmidhuber, "World Models", 2018
- **A2C**: Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning", 2016
- **GAE**: Schulman et al., "High-Dimensional Continuous Control Using Generalized Advantage Estimation", 2015

---

## License

Same as the original project.

---

## Contact

For issues or questions about these improvements, please open a GitHub issue.
