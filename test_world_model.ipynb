{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Enhanced World Model - Testing Notebook\n",
    "\n",
    "This notebook demonstrates the improvements made to the world model.\n",
    "\n",
    "**What's New:**\n",
    "- ‚úÖ A2C training with proper advantages (GAE)\n",
    "- ‚úÖ Improved MLP controllers with planning\n",
    "- ‚úÖ Fixed gradient flow in memory model\n",
    "- ‚úÖ Memory prediction loss\n",
    "- ‚úÖ Debug output enabled\n",
    "\n",
    "**Runtime:** Use GPU runtime for faster training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Enable immediate output flushing\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Using CPU (training will be slower)\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install gymnasium[classic-control] tensorboard pygame swig\n",
    "# !pip install gymnasium[box2d]\n",
    "# !pip install matplotlib opencv-python\n",
    "\n",
    "print(\"Dependencies ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# Add src to path if not already there\n",
    "src_path = os.path.join(current_dir, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"Added {src_path} to Python path\")\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from WorldModel import WorldModel\n",
    "    from train_a2c import train_a2c\n",
    "    print(\"‚úÖ Successfully imported world model components\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import sys\n",
    "\n",
    "# Enable immediate output flushing for debug prints\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# Import world model components\n",
    "from vision.VQ_VAE import VQ_VAE\n",
    "from vision.Identity import Identity\n",
    "from memory.TemporalTransformer import TemporalTransformer\n",
    "from controller.DiscreteModelPredictiveController import DiscreteModelPredictiveController\n",
    "from controller.ContinuousModelPredictiveController import ContinuousModelPredictiveController\n",
    "from controller.ImprovedDiscreteController import ImprovedDiscreteController\n",
    "from controller.ImprovedContinuousController import ImprovedContinuousController\n",
    "from WorldModel import WorldModel\n",
    "from train_a2c import train_a2c\n",
    "from reward_predictor.LinearPredictor import LinearPredictorModel\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(\"Debug output is ENABLED - you will see [DEBUG] messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ENV_NAME = \"CartPole-v1\"  # Change to \"CarRacing-v3\" for visual environment\n",
    "NUM_ENVS = 4  # Parallel environments\n",
    "MAX_EPOCHS = 50  # Increase to 200+ for full training\n",
    "LEARNING_RATE = 3e-4\n",
    "N_STEPS = 128  # Steps per A2C update\n",
    "PLANNING_HORIZON = 5\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Parallel envs: {NUM_ENVS}\")\n",
    "print(f\"Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Planning horizon: {PLANNING_HORIZON}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_world_model(env_name, num_envs, use_improved_controller=True):\n",
    "    \"\"\"Create a world model for the given environment.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Creating World Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create environment to inspect spaces\n",
    "    envs = gym.make_vec(env_name, num_envs=num_envs, render_mode='rgb_array')\n",
    "    obs_space = envs.single_observation_space\n",
    "    action_space = envs.single_action_space\n",
    "    \n",
    "    print(f\"Environment: {env_name}\")\n",
    "    print(f\"Observation space: {obs_space}\")\n",
    "    print(f\"Action space: {action_space}\")\n",
    "    \n",
    "    is_image_based = len(obs_space.shape) == 3\n",
    "    \n",
    "    # Configure vision model\n",
    "    if is_image_based:\n",
    "        obs_shape = obs_space.shape\n",
    "        input_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # (C, H, W)\n",
    "        vision_model = VQ_VAE\n",
    "        vision_args = {\"output_dim\": input_shape[0], \"embed_dim\": 64}\n",
    "        print(f\"Vision: VQ_VAE (image-based)\")\n",
    "    else:\n",
    "        input_shape = obs_space.shape\n",
    "        vision_model = Identity\n",
    "        vision_args = {\"embed_dim\": obs_space.shape[0]}\n",
    "        print(f\"Vision: Identity (state-based)\")\n",
    "    \n",
    "    # Configure controller\n",
    "    if isinstance(action_space, gym.spaces.Discrete):\n",
    "        action_dim = action_space.n\n",
    "        if use_improved_controller:\n",
    "            controller_model = ImprovedDiscreteController\n",
    "            controller_args = {\n",
    "                \"action_dim\": action_dim,\n",
    "                \"use_planning\": True,\n",
    "                \"planning_horizon\": PLANNING_HORIZON\n",
    "            }\n",
    "            print(f\"Controller: ImprovedDiscreteController (with planning)\")\n",
    "        else:\n",
    "            controller_model = DiscreteModelPredictiveController\n",
    "            controller_args = {\"action_dim\": action_dim}\n",
    "            print(f\"Controller: DiscreteModelPredictiveController (legacy)\")\n",
    "    else:\n",
    "        action_dim = action_space.shape[0]\n",
    "        if use_improved_controller:\n",
    "            controller_model = ImprovedContinuousController\n",
    "            controller_args = {\n",
    "                \"action_dim\": action_dim,\n",
    "                \"use_planning\": True,\n",
    "                \"planning_horizon\": PLANNING_HORIZON\n",
    "            }\n",
    "            print(f\"Controller: ImprovedContinuousController (with planning)\")\n",
    "        else:\n",
    "            controller_model = ContinuousModelPredictiveController\n",
    "            controller_args = {\"action_dim\": action_dim}\n",
    "            print(f\"Controller: ContinuousModelPredictiveController (legacy)\")\n",
    "    \n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    \n",
    "    # Configure memory\n",
    "    memory_args = {\n",
    "        \"d_model\": 128,\n",
    "        \"latent_dim\": vision_args[\"embed_dim\"],\n",
    "        \"action_dim\": action_dim,\n",
    "        \"nhead\": 8\n",
    "    }\n",
    "    print(f\"Memory: TemporalTransformer (d_model={memory_args['d_model']})\")\n",
    "    \n",
    "    # Create world model\n",
    "    print(\"\\nInitializing world model...\")\n",
    "    world_model = WorldModel(\n",
    "        vision_model=vision_model,\n",
    "        memory_model=TemporalTransformer,\n",
    "        controller_model=controller_model,\n",
    "        input_shape=input_shape,\n",
    "        vision_args=vision_args,\n",
    "        memory_args=memory_args,\n",
    "        controller_args=controller_args,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Add reward predictor\n",
    "    world_model.set_reward_predictor(LinearPredictorModel)\n",
    "    print(\"Reward predictor: LinearPredictorModel\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in world_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in world_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ World Model Created Successfully\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return world_model, envs\n",
    "\n",
    "\n",
    "def plot_training_results(rewards, title=\"Training Progress\"):\n",
    "    \"\"\"Plot training results.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards, alpha=0.7)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = 10\n",
    "    if len(rewards) >= window:\n",
    "        smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smooth, linewidth=2)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title(f'Smoothed Progress (window={window})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nüìä Training Statistics:\")\n",
    "    print(f\"Mean Reward: {np.mean(rewards):.2f}\")\n",
    "    print(f\"Max Reward: {np.max(rewards):.2f}\")\n",
    "    print(f\"Min Reward: {np.min(rewards):.2f}\")\n",
    "    print(f\"Std Reward: {np.std(rewards):.2f}\")\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test 1: Quick Sanity Check\n",
    "\n",
    "Verify the improved controller can forward pass and plan. **You should see DEBUG output here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SANITY CHECK - Testing Forward Pass\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Creating world model...\")\n",
    "model, envs = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "state, _ = envs.reset()\n",
    "is_image_based = len(envs.single_observation_space.shape) == 3\n",
    "\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"Is image based: {is_image_based}\")\n",
    "\n",
    "if is_image_based:\n",
    "    state_tensor = torch.from_numpy(state.transpose(0, 3, 1, 2)).float().to(device) / 255.0\n",
    "else:\n",
    "    state_tensor = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "print(f\"State tensor shape: {state_tensor.shape}\")\n",
    "print(\"\\nPerforming forward pass (watch for DEBUG output)...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(state_tensor, action_space=envs.single_action_space,\n",
    "                   is_image_based=is_image_based, return_losses=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Forward pass successful!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Action shape: {output['action'].shape}\")\n",
    "print(f\"Action value: {output['action']}\")\n",
    "print(f\"Value estimate: {output['value'].item():.4f}\")\n",
    "print(f\"Log probability: {output['log_probs'].item():.4f}\")\n",
    "print(f\"Reconstruction loss: {output['recon_loss'].mean().item():.4f}\")\n",
    "print(f\"VQ loss: {output['vq_loss'].mean().item():.4f}\")\n",
    "print(f\"Total loss: {output['total_loss'].item():.4f}\")\n",
    "\n",
    "# Test planning (if available)\n",
    "if hasattr(model.controller, 'use_planning') and model.controller.use_planning:\n",
    "    print(f\"\\n‚úÖ Planning enabled with horizon: {model.controller.planning_horizon}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Legacy controller (no planning)\")\n",
    "\n",
    "envs.close()\n",
    "print(\"\\n‚úÖ Sanity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Test 2: Train with A2C (New System)\n",
    "\n",
    "Train using the improved A2C algorithm with proper advantages.\n",
    "\n",
    "**Note:** Debug output will be visible during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TRAINING WITH A2C (NEW SYSTEM)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Create model with improved controller\n",
    "model_new, envs_new = create_world_model(ENV_NAME, num_envs=NUM_ENVS, use_improved_controller=True)\n",
    "\n",
    "# Create directory for checkpoints\n",
    "import os\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "print(\"Checkpoints directory: ./checkpoints/\\n\")\n",
    "\n",
    "# Train with A2C\n",
    "print(f\"Starting A2C training for {MAX_EPOCHS} epochs...\")\n",
    "print(\"Watch for DEBUG output during training!\\n\")\n",
    "\n",
    "try:\n",
    "    train_a2c(\n",
    "        model=model_new,\n",
    "        envs=envs_new,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        n_steps=N_STEPS,\n",
    "        device=device,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        memory_coef=0.1,\n",
    "        max_grad_norm=0.5,\n",
    "        use_tensorboard=False,  # Disable for notebook\n",
    "        save_path='./checkpoints/',\n",
    "        save_prefix='a2c_notebook'\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "finally:\n",
    "    envs_new.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ A2C Training complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ Checkpoints saved to ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Test 3: Evaluate Trained Model\n",
    "\n",
    "Evaluate the trained model and visualize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATING TRAINED MODEL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Load best model\n",
    "model_eval, envs_eval = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "try:\n",
    "    checkpoint_path = f'./checkpoints/a2c_notebook_{ENV_NAME}_best.pt'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model_eval.load(checkpoint_path, \n",
    "                       obs_space=envs_eval.single_observation_space,\n",
    "                       action_space=envs_eval.single_action_space)\n",
    "        print(f\"‚úÖ Loaded checkpoint: {checkpoint_path}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No checkpoint found, using current model\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load checkpoint: {e}\\n\")\n",
    "\n",
    "model_eval.eval()\n",
    "\n",
    "# Run evaluation episodes\n",
    "num_eval_episodes = 10\n",
    "eval_rewards = []\n",
    "eval_lengths = []\n",
    "\n",
    "is_image_based = len(envs_eval.single_observation_space.shape) == 3\n",
    "\n",
    "print(f\"Running {num_eval_episodes} evaluation episodes...\\n\")\n",
    "\n",
    "for ep in range(num_eval_episodes):\n",
    "    state, _ = envs_eval.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Prepare state\n",
    "        if is_image_based:\n",
    "            state_tensor = torch.from_numpy(state.transpose(0, 3, 1, 2)).float().to(device) / 255.0\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        # Get action (deterministic)\n",
    "        with torch.no_grad():\n",
    "            action = model_eval(state_tensor, \n",
    "                               action_space=envs_eval.single_action_space,\n",
    "                               is_image_based=is_image_based)\n",
    "        \n",
    "        # Execute action\n",
    "        if isinstance(envs_eval.single_action_space, gym.spaces.Discrete):\n",
    "            action_np = action.cpu().numpy()\n",
    "        else:\n",
    "            action_np = action.cpu().numpy()\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = envs_eval.step(action_np)\n",
    "        done = terminated[0] or truncated[0]\n",
    "        total_reward += reward[0]\n",
    "        steps += 1\n",
    "    \n",
    "    eval_rewards.append(total_reward)\n",
    "    eval_lengths.append(steps)\n",
    "    print(f\"Episode {ep+1}/{num_eval_episodes}: Reward = {total_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    # Reset memory for next episode\n",
    "    model_eval.reset_env_memory(0)\n",
    "\n",
    "envs_eval.close()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Reward: {np.mean(eval_rewards):.2f} ¬± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"Max Reward: {np.max(eval_rewards):.2f}\")\n",
    "print(f\"Min Reward: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"Mean Episode Length: {np.mean(eval_lengths):.1f} ¬± {np.std(eval_lengths):.1f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(eval_rewards)), eval_rewards, alpha=0.7)\n",
    "plt.axhline(np.mean(eval_rewards), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation Episode Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(eval_lengths)), eval_lengths, alpha=0.7, color='green')\n",
    "plt.axhline(np.mean(eval_lengths), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.title('Evaluation Episode Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Test 4: Inspect Model Architecture\n",
    "\n",
    "Detailed inspection of the model components and their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç MODEL ARCHITECTURE INSPECTION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Create a fresh model for inspection\n",
    "model_inspect, envs_inspect = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nüìê Parameter Counts by Component:\")\n",
    "print(f\"Vision: {count_parameters(model_inspect.vision):,}\")\n",
    "print(f\"Memory: {count_parameters(model_inspect.memory):,}\")\n",
    "print(f\"Controller: {count_parameters(model_inspect.controller):,}\")\n",
    "if model_inspect.reward_predictor:\n",
    "    print(f\"Reward Predictor: {count_parameters(model_inspect.reward_predictor):,}\")\n",
    "print(f\"\\nTotal: {count_parameters(model_inspect):,}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è Controller Architecture:\")\n",
    "print(model_inspect.controller)\n",
    "\n",
    "print(\"\\nüß† Memory Architecture:\")\n",
    "print(model_inspect.memory)\n",
    "\n",
    "print(\"\\nüëÅÔ∏è Vision Architecture:\")\n",
    "print(model_inspect.vision)\n",
    "\n",
    "if model_inspect.reward_predictor:\n",
    "    print(\"\\nüéÅ Reward Predictor Architecture:\")\n",
    "    print(model_inspect.reward_predictor)\n",
    "\n",
    "envs_inspect.close()\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "### What We Tested:\n",
    "1. ‚úÖ Model creation and forward pass with DEBUG output\n",
    "2. ‚úÖ A2C training with improved controller\n",
    "3. ‚úÖ Evaluation and performance metrics\n",
    "4. ‚úÖ Architecture inspection\n",
    "\n",
    "### Key Improvements:\n",
    "- **Controller**: Single linear layer ‚Üí Multi-layer MLP with planning\n",
    "- **Training**: Simple REINFORCE ‚Üí A2C with GAE\n",
    "- **Gradient Flow**: Detached memory ‚Üí Proper gradient flow\n",
    "- **Losses**: Vision only ‚Üí Vision + Memory + Policy + Value\n",
    "- **Debug Output**: Now properly visible in notebook!\n",
    "\n",
    "### Expected Performance (CartPole-v1):\n",
    "- **Goal**: Mean reward 400-500 (solves environment)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Longer Training**: Increase `MAX_EPOCHS` to 200+ for full convergence\n",
    "2. **Different Environments**: Try `CarRacing-v3` or other environments\n",
    "3. **Hyperparameter Tuning**: Adjust learning rate, n_steps, planning horizon\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
