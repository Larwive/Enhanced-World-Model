{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üß† Enhanced World Model - Testing Notebook\n",
    "\n",
    "This notebook demonstrates the improvements made to the world model and allows you to compare old vs new systems.\n",
    "\n",
    "**What's New:**\n",
    "- ‚úÖ A2C training with proper advantages (GAE)\n",
    "- ‚úÖ Improved MLP controllers with planning\n",
    "- ‚úÖ Fixed gradient flow in memory model\n",
    "- ‚úÖ Memory prediction loss\n",
    "- ‚úÖ Stable, effective learning\n",
    "\n",
    "**Runtime:** Use GPU runtime for faster training!\n",
    "- Runtime ‚Üí Change runtime type ‚Üí GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Using CPU (training will be slower)\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install gymnasium[classic-control] tensorboard pygame swig\n",
    "\n",
    "# For CarRacing environment\n",
    "!pip install gymnasium[box2d]\n",
    "\n",
    "# Optional: For visualization\n",
    "!pip install matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# If running in Colab, clone the repository\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists('Enhanced-World-Model'):\n",
    "        # Clone your repo (replace with your actual repo URL)\n",
    "        !git clone https://github.com/YOUR_USERNAME/Enhanced-World-Model.git\n",
    "    os.chdir('Enhanced-World-Model')\n",
    "    print(\"Working directory:\", os.getcwd())\n",
    "else:\n",
    "    # Running locally\n",
    "    print(\"Running locally\")\n",
    "    # Make sure we're in the right directory\n",
    "    if 'Enhanced-World-Model' not in os.getcwd():\n",
    "        print(\"Please run this notebook from the Enhanced-World-Model directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "add_to_path"
   },
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from WorldModel import WorldModel\n",
    "    from train_a2c import train_a2c\n",
    "    print(\"‚úÖ Successfully imported world model components\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Make sure you're in the correct directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Import world model components\n",
    "from vision.VQ_VAE import VQ_VAE\n",
    "from vision.Identity import Identity\n",
    "from memory.TemporalTransformer import TemporalTransformer\n",
    "from controller.DiscreteModelPredictiveController import DiscreteModelPredictiveController\n",
    "from controller.ContinuousModelPredictiveController import ContinuousModelPredictiveController\n",
    "from controller.ImprovedDiscreteController import ImprovedDiscreteController\n",
    "from controller.ImprovedContinuousController import ImprovedContinuousController\n",
    "from WorldModel import WorldModel\n",
    "from train import train as train_legacy\n",
    "from train_a2c import train_a2c\n",
    "from reward_predictor.LinearPredictor import LinearPredictorModel\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_params"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ENV_NAME = \"CartPole-v1\"  # Change to \"CarRacing-v3\" for visual environment\n",
    "NUM_ENVS = 4  # Parallel environments\n",
    "MAX_EPOCHS = 50  # Increase to 200+ for full training\n",
    "LEARNING_RATE = 3e-4\n",
    "N_STEPS = 128  # Steps per A2C update\n",
    "PLANNING_HORIZON = 5\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Parallel envs: {NUM_ENVS}\")\n",
    "print(f\"Max epochs: {MAX_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "helper"
   },
   "source": [
    "## üõ†Ô∏è Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helpers"
   },
   "outputs": [],
   "source": [
    "def create_world_model(env_name, num_envs, use_improved_controller=True):\n",
    "    \"\"\"Create a world model for the given environment.\"\"\"\n",
    "    \n",
    "    # Create environment to inspect spaces\n",
    "    envs = gym.make_vec(env_name, num_envs=num_envs, render_mode='rgb_array')\n",
    "    obs_space = envs.single_observation_space\n",
    "    action_space = envs.single_action_space\n",
    "    \n",
    "    is_image_based = len(obs_space.shape) == 3\n",
    "    \n",
    "    # Configure vision model\n",
    "    if is_image_based:\n",
    "        obs_shape = obs_space.shape\n",
    "        input_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # (C, H, W)\n",
    "        vision_model = VQ_VAE\n",
    "        vision_args = {\"output_dim\": input_shape[0], \"embed_dim\": 64}\n",
    "    else:\n",
    "        input_shape = obs_space.shape\n",
    "        vision_model = Identity\n",
    "        vision_args = {\"embed_dim\": obs_space.shape[0]}\n",
    "    \n",
    "    # Configure controller\n",
    "    if isinstance(action_space, gym.spaces.Discrete):\n",
    "        action_dim = action_space.n\n",
    "        if use_improved_controller:\n",
    "            controller_model = ImprovedDiscreteController\n",
    "            controller_args = {\n",
    "                \"action_dim\": action_dim,\n",
    "                \"use_planning\": True,\n",
    "                \"planning_horizon\": PLANNING_HORIZON\n",
    "            }\n",
    "        else:\n",
    "            controller_model = DiscreteModelPredictiveController\n",
    "            controller_args = {\"action_dim\": action_dim}\n",
    "    else:\n",
    "        action_dim = action_space.shape[0]\n",
    "        if use_improved_controller:\n",
    "            controller_model = ImprovedContinuousController\n",
    "            controller_args = {\n",
    "                \"action_dim\": action_dim,\n",
    "                \"use_planning\": True,\n",
    "                \"planning_horizon\": PLANNING_HORIZON\n",
    "            }\n",
    "        else:\n",
    "            controller_model = ContinuousModelPredictiveController\n",
    "            controller_args = {\"action_dim\": action_dim}\n",
    "    \n",
    "    # Configure memory\n",
    "    memory_args = {\n",
    "        \"d_model\": 128,\n",
    "        \"latent_dim\": vision_args[\"embed_dim\"],\n",
    "        \"action_dim\": action_dim,\n",
    "        \"nhead\": 8\n",
    "    }\n",
    "    \n",
    "    # Create world model\n",
    "    world_model = WorldModel(\n",
    "        vision_model=vision_model,\n",
    "        memory_model=TemporalTransformer,\n",
    "        controller_model=controller_model,\n",
    "        input_shape=input_shape,\n",
    "        vision_args=vision_args,\n",
    "        memory_args=memory_args,\n",
    "        controller_args=controller_args,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Add reward predictor\n",
    "    world_model.set_reward_predictor(LinearPredictorModel)\n",
    "    \n",
    "    controller_type = \"Improved\" if use_improved_controller else \"Legacy\"\n",
    "    print(f\"‚úÖ Created {controller_type} World Model\")\n",
    "    print(f\"   Vision: {vision_model.__name__}\")\n",
    "    print(f\"   Controller: {controller_model.__name__}\")\n",
    "    print(f\"   Total parameters: {sum(p.numel() for p in world_model.parameters()):,}\")\n",
    "    \n",
    "    return world_model, envs\n",
    "\n",
    "\n",
    "def plot_training_results(rewards_old, rewards_new):\n",
    "    \"\"\"Plot comparison of old vs new training.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_old, label='Old System', alpha=0.7)\n",
    "    plt.plot(rewards_new, label='New System (A2C)', alpha=0.7)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Training Progress Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = 10\n",
    "    if len(rewards_old) >= window:\n",
    "        old_smooth = np.convolve(rewards_old, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(old_smooth, label='Old System (smoothed)', linewidth=2)\n",
    "    if len(rewards_new) >= window:\n",
    "        new_smooth = np.convolve(rewards_new, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(new_smooth, label='New System (smoothed)', linewidth=2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title(f'Smoothed Progress (window={window})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nüìä Training Statistics:\")\n",
    "    print(f\"Old System - Mean: {np.mean(rewards_old):.2f}, Max: {np.max(rewards_old):.2f}\")\n",
    "    print(f\"New System - Mean: {np.mean(rewards_new):.2f}, Max: {np.max(rewards_new):.2f}\")\n",
    "    print(f\"Improvement: {((np.mean(rewards_new) - np.mean(rewards_old)) / max(abs(np.mean(rewards_old)), 1e-6) * 100):.1f}%\")\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test1"
   },
   "source": [
    "## üß™ Test 1: Quick Sanity Check\n",
    "\n",
    "Let's verify the improved controller can forward pass and plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sanity_check"
   },
   "outputs": [],
   "source": [
    "print(\"Creating world model...\")\n",
    "model, envs = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "state, _ = envs.reset()\n",
    "is_image_based = len(envs.single_observation_space.shape) == 3\n",
    "\n",
    "if is_image_based:\n",
    "    state_tensor = torch.from_numpy(state.transpose(0, 3, 1, 2)).float().to(device) / 255.0\n",
    "else:\n",
    "    state_tensor = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(state_tensor, action_space=envs.single_action_space,\n",
    "                   is_image_based=is_image_based, return_losses=True)\n",
    "\n",
    "print(f\"‚úÖ Forward pass successful!\")\n",
    "print(f\"   Action shape: {output['action'].shape}\")\n",
    "print(f\"   Value: {output['value'].item():.4f}\")\n",
    "print(f\"   Log prob: {output['log_probs'].item():.4f}\")\n",
    "print(f\"   Vision loss: {output['total_loss'].item():.4f}\")\n",
    "\n",
    "# Test planning (if available)\n",
    "if hasattr(model.controller, 'use_planning'):\n",
    "    print(f\"\\n‚úÖ Planning enabled with horizon: {model.controller.planning_horizon}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Legacy controller (no planning)\")\n",
    "\n",
    "envs.close()\n",
    "print(\"\\n‚úÖ Sanity check passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test2"
   },
   "source": [
    "## üèãÔ∏è Test 2: Train with A2C (New System)\n",
    "\n",
    "Train using the improved A2C algorithm with proper advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_a2c"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üöÄ TRAINING WITH A2C (NEW SYSTEM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model with improved controller\n",
    "model_new, envs_new = create_world_model(ENV_NAME, num_envs=NUM_ENVS, use_improved_controller=True)\n",
    "\n",
    "# Create directory for checkpoints\n",
    "import os\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "# Train with A2C\n",
    "print(f\"\\nStarting A2C training for {MAX_EPOCHS} epochs...\\n\")\n",
    "\n",
    "try:\n",
    "    train_a2c(\n",
    "        model=model_new,\n",
    "        envs=envs_new,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        n_steps=N_STEPS,\n",
    "        device=device,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        memory_coef=0.1,\n",
    "        max_grad_norm=0.5,\n",
    "        use_tensorboard=False,  # Disable for notebook\n",
    "        save_path='./checkpoints/',\n",
    "        save_prefix='a2c_notebook'\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "finally:\n",
    "    envs_new.close()\n",
    "\n",
    "print(\"\\n‚úÖ A2C Training complete!\")\n",
    "print(\"üìÅ Checkpoints saved to ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test3"
   },
   "source": [
    "## üìä Test 3: Evaluate Trained Model\n",
    "\n",
    "Evaluate the trained model and visualize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATING TRAINED MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "model_eval, envs_eval = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "try:\n",
    "    checkpoint_path = f'./checkpoints/a2c_notebook_{ENV_NAME}_best.pt'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model_eval.load(checkpoint_path, \n",
    "                       obs_space=envs_eval.single_observation_space,\n",
    "                       action_space=envs_eval.single_action_space)\n",
    "        print(f\"‚úÖ Loaded checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No checkpoint found, using current model\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load checkpoint: {e}\")\n",
    "\n",
    "model_eval.eval()\n",
    "\n",
    "# Run evaluation episodes\n",
    "num_eval_episodes = 10\n",
    "eval_rewards = []\n",
    "eval_lengths = []\n",
    "\n",
    "is_image_based = len(envs_eval.single_observation_space.shape) == 3\n",
    "\n",
    "print(f\"\\nRunning {num_eval_episodes} evaluation episodes...\")\n",
    "\n",
    "for ep in range(num_eval_episodes):\n",
    "    state, _ = envs_eval.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Prepare state\n",
    "        if is_image_based:\n",
    "            state_tensor = torch.from_numpy(state.transpose(0, 3, 1, 2)).float().to(device) / 255.0\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        # Get action (deterministic)\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model_eval.controller, 'use_planning'):\n",
    "                # Use deterministic action\n",
    "                action, _, _, _ = model_eval.controller(z_t=None, h_t=None, deterministic=True)\n",
    "                # But we need to do a full forward pass for proper latents\n",
    "                action = model_eval(state_tensor, \n",
    "                                   action_space=envs_eval.single_action_space,\n",
    "                                   is_image_based=is_image_based)\n",
    "            else:\n",
    "                action = model_eval(state_tensor,\n",
    "                                   action_space=envs_eval.single_action_space,\n",
    "                                   is_image_based=is_image_based)\n",
    "        \n",
    "        # Execute action\n",
    "        if isinstance(envs_eval.single_action_space, gym.spaces.Discrete):\n",
    "            action_np = action.cpu().numpy()\n",
    "        else:\n",
    "            action_np = action.cpu().numpy()\n",
    "        \n",
    "        state, reward, terminated, truncated, _ = envs_eval.step(action_np)\n",
    "        done = terminated[0] or truncated[0]\n",
    "        total_reward += reward[0]\n",
    "        steps += 1\n",
    "    \n",
    "    eval_rewards.append(total_reward)\n",
    "    eval_lengths.append(steps)\n",
    "    print(f\"Episode {ep+1}/{num_eval_episodes}: Reward = {total_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    # Reset memory for next episode\n",
    "    model_eval.reset_env_memory(0)\n",
    "\n",
    "envs_eval.close()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Reward: {np.mean(eval_rewards):.2f} ¬± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"Max Reward: {np.max(eval_rewards):.2f}\")\n",
    "print(f\"Min Reward: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"Mean Episode Length: {np.mean(eval_lengths):.1f} ¬± {np.std(eval_lengths):.1f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(eval_rewards)), eval_rewards, alpha=0.7)\n",
    "plt.axhline(np.mean(eval_rewards), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation Episode Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(eval_lengths)), eval_lengths, alpha=0.7, color='green')\n",
    "plt.axhline(np.mean(eval_lengths), color='r', linestyle='--', label='Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.title('Evaluation Episode Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test4"
   },
   "source": [
    "## üéÆ Test 4: Visualize Agent Behavior (Optional)\n",
    "\n",
    "Watch the trained agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# This section renders the environment\n",
    "# Note: May not work perfectly in all Colab environments\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Rendering trained agent...\")\n",
    "\n",
    "# Create environment with rendering\n",
    "env_render = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "model_render, _ = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "try:\n",
    "    checkpoint_path = f'./checkpoints/a2c_notebook_{ENV_NAME}_best.pt'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model_render.load(checkpoint_path,\n",
    "                         obs_space=env_render.observation_space,\n",
    "                         action_space=env_render.action_space)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model_render.eval()\n",
    "is_image_based = len(env_render.observation_space.shape) == 3\n",
    "\n",
    "# Run one episode with rendering\n",
    "state, _ = env_render.reset()\n",
    "frames = []\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done and len(frames) < 500:  # Max 500 frames\n",
    "    # Render\n",
    "    frame = env_render.render()\n",
    "    frames.append(frame)\n",
    "    \n",
    "    # Get action\n",
    "    if is_image_based:\n",
    "        state_tensor = torch.from_numpy(state.transpose(2, 0, 1)).float().unsqueeze(0).to(device) / 255.0\n",
    "    else:\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action = model_render(state_tensor,\n",
    "                             action_space=env_render.action_space,\n",
    "                             is_image_based=is_image_based)\n",
    "    \n",
    "    if isinstance(env_render.action_space, gym.spaces.Discrete):\n",
    "        action_np = action.cpu().numpy()[0]\n",
    "    else:\n",
    "        action_np = action.cpu().numpy()[0]\n",
    "    \n",
    "    state, reward, terminated, truncated, _ = env_render.step(action_np)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "env_render.close()\n",
    "\n",
    "print(f\"Collected {len(frames)} frames, Total reward: {total_reward:.2f}\")\n",
    "\n",
    "# Display some frames\n",
    "if len(frames) > 0:\n",
    "    fig, axes = plt.subplots(1, min(5, len(frames)), figsize=(15, 3))\n",
    "    if min(5, len(frames)) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    step_indices = np.linspace(0, len(frames)-1, min(5, len(frames)), dtype=int)\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.imshow(frames[step_indices[idx]])\n",
    "        ax.set_title(f\"Step {step_indices[idx]}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Tip: For full video, you can save frames to a video file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## üìä Test 5: Compare Old vs New System (Optional)\n",
    "\n",
    "Direct comparison between legacy and improved training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare"
   },
   "outputs": [],
   "source": [
    "# This is optional and will take longer\n",
    "# Uncomment to run comparison\n",
    "\n",
    "# COMPARISON_EPOCHS = 20\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"‚öîÔ∏è OLD vs NEW SYSTEM COMPARISON\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Train old system\n",
    "# print(\"\\n1Ô∏è‚É£ Training OLD system (legacy)...\")\n",
    "# model_old, envs_old = create_world_model(ENV_NAME, num_envs=NUM_ENVS, use_improved_controller=False)\n",
    "# # Would need to track rewards in train_legacy function\n",
    "# # This is left as an exercise\n",
    "\n",
    "# # Train new system\n",
    "# print(\"\\n2Ô∏è‚É£ Training NEW system (A2C)...\")\n",
    "# model_new, envs_new = create_world_model(ENV_NAME, num_envs=NUM_ENVS, use_improved_controller=True)\n",
    "# # Would need to track rewards in train_a2c function\n",
    "# # This is left as an exercise\n",
    "\n",
    "# print(\"\\n‚ö†Ô∏è Full comparison requires modifying training loops to return reward history\")\n",
    "# print(\"See IMPROVEMENTS.md for expected performance improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis"
   },
   "source": [
    "## üîç Analysis: Inspect Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inspect"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üîç MODEL ARCHITECTURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create both models for comparison\n",
    "model_old, envs_old = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=False)\n",
    "model_new, envs_new = create_world_model(ENV_NAME, num_envs=1, use_improved_controller=True)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nüìê Parameter Counts:\")\n",
    "print(f\"Vision (both): {count_parameters(model_new.vision):,}\")\n",
    "print(f\"Memory (both): {count_parameters(model_new.memory):,}\")\n",
    "print(f\"Controller (OLD): {count_parameters(model_old.controller):,}\")\n",
    "print(f\"Controller (NEW): {count_parameters(model_new.controller):,}\")\n",
    "print(f\"\\nTotal (OLD): {count_parameters(model_old):,}\")\n",
    "print(f\"Total (NEW): {count_parameters(model_new):,}\")\n",
    "print(f\"Increase: {count_parameters(model_new) - count_parameters(model_old):,} parameters\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è Controller Architecture:\")\n",
    "print(\"\\nOLD Controller:\")\n",
    "print(model_old.controller)\n",
    "print(\"\\nNEW Controller:\")\n",
    "print(model_new.controller)\n",
    "\n",
    "envs_old.close()\n",
    "envs_new.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìù Summary & Next Steps\n",
    "\n",
    "### What We Tested:\n",
    "1. ‚úÖ Model creation and forward pass\n",
    "2. ‚úÖ A2C training with improved controller\n",
    "3. ‚úÖ Evaluation and performance metrics\n",
    "4. ‚úÖ Visualization of trained agent\n",
    "5. ‚úÖ Architecture comparison\n",
    "\n",
    "### Key Improvements:\n",
    "- **Controller**: Single linear layer ‚Üí Multi-layer MLP with planning\n",
    "- **Training**: Simple REINFORCE ‚Üí A2C with GAE\n",
    "- **Gradient Flow**: Detached memory ‚Üí Proper gradient flow\n",
    "- **Losses**: Vision only ‚Üí Vision + Memory + Policy + Value\n",
    "\n",
    "### Expected Performance (CartPole-v1):\n",
    "- **Old System**: Mean reward 20-50\n",
    "- **New System**: Mean reward 400-500 (solves environment)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Longer Training**: Increase `MAX_EPOCHS` to 200+ for full convergence\n",
    "2. **Different Environments**: Try `CarRacing-v3` or other environments\n",
    "3. **Hyperparameter Tuning**: Adjust learning rate, n_steps, planning horizon\n",
    "4. **Pre-training**: Use vision/memory pre-training for complex environments\n",
    "\n",
    "### Save Your Work:\n",
    "```python\n",
    "# Download checkpoints from Colab\n",
    "from google.colab import files\n",
    "files.download('./checkpoints/a2c_notebook_CartPole-v1_best.pt')\n",
    "```\n",
    "\n",
    "### Resources:\n",
    "- See `IMPROVEMENTS.md` for detailed documentation\n",
    "- Check TensorBoard logs for training curves (if enabled)\n",
    "- Refer to `src/train_a2c.py` for training algorithm details\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
